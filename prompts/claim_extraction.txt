
-Target activity-
You are an intelligent assistant that helps a human analyst to analyze claims associated with specific scientific concepts, technologies, and research methodologies presented in a scientific text document.

-Goal-
Given a scientific text document that is potentially relevant to this activity, an entity specification, and a claim description, extract all entities that match the entity specification and all claims related to those entities.

-Steps-
1. Extract all named entities that match the predefined entity specification. The entity specification can either be a list of entity names or a list of entity types.
2.  For each entity identified in step 1, extract all claims associated with the entity. Claims need to match the specified claim description, and the entity should be the subject of the claim.
- Subject: The name of the entity that is the subject of the claim, capitalized. The subject entity is the one that is involved in or responsible for the action described in the claim. The subject must be one of the named entities identified in step 1.
- Object: The name of the entity that is the object of the claim, capitalized. The object entity is one that is affected by or is the target of the action described in the claim. If the object entity is unknown, use NONE.
- Claim Type: The overall category of the claim, capitalized. Name it in a way that can be repeated across multiple text inputs, so that similar claims share the same claim type.
- Claim Status: **TRUE**, **FALSE**, or **SUSPECTED**. TRUE means the claim is confirmed, FALSE means the claim is found to be False, SUSPECTED means the claim is not verified.
- Claim Description:  A detailed description explaining the reasoning behind the claim, together with all related evidence and references.
- Claim Date: Period (start_date, end_date) when the claim was made. Both start_date and end_date should be in ISO-8601 format. If the claim was made on a single date rather than a date range, set the same date for both start_date and end_date. If date is unknown, return **NONE**.
- Claim Source Text: List of **all** quotes from the original text that are relevant to the claim.

Format each claim as (<subject_entity>{tuple_delimiter}<object_entity>{tuple_delimiter}<claim_type>{tuple_delimiter}<claim_status>{tuple_delimiter}<claim_start_date>{tuple_delimiter}<claim_end_date>{tuple_delimiter}<claim_description>{tuple_delimiter}<claim_source>)

3. Return output in English as a single list of all the claims identified in steps 1 and 2. Use **{record_delimiter}** as the list delimiter.

4. When finished, output {completion_delimiter}

-Examples-
Example 1:
Entity specification: [Knowledge, Technology, Methodology, Theory, Experiment, Hypothesis, Research Field, Tool, Dataset, Algorithm, Model]
Claim description: Efficiency improvements in AI models
Text: In recent research, the application of Low-Rank Adaptation (LoRA) to large language models has been demonstrated to significantly reduce computational costs. A study published in 2023 by Dr. Kim et al. at the Computational Linguistics Institute showed that LoRA reduces the number of trainable parameters in the GPT-4 model by 85%, while maintaining 95% of the original model's accuracy on the SQuAD dataset.
Output:

(LOW-RANK ADAPTATION (LORA){tuple_delimiter}GPT-4{tuple_delimiter}EFFICIENCY IMPROVEMENTS{tuple_delimiter}TRUE{tuple_delimiter}2023-01-01T00:00:00{tuple_delimiter}2023-12-31T00:00:00{tuple_delimiter}Low-Rank Adaptation (LoRA) was shown to reduce the number of trainable parameters in GPT-4 by 85%, while maintaining 95% of the original model's accuracy on the SQuAD dataset according to a study by Dr. Kim et al. in 2023{tuple_delimiter}In recent research, the application of Low-Rank Adaptation (LoRA) to large language models has been demonstrated to significantly reduce computational costs.)

Example 2:
Entity specification: [Knowledge, Technology, Methodology, Theory, Experiment, Hypothesis, Research Field, Tool, Dataset, Algorithm, Model]
Claim description: Accuracy improvements in natural language processing models
Text: The new fine-tuning method, Parameter-Efficient Fine-Tuning (PEFT), has shown remarkable improvements in natural language processing tasks. A 2024 paper by the AI Research Lab reported that PEFT improved the accuracy of BERT models on the GLUE benchmark by 3%, outperforming previous state-of-the-art methods.
Output:

(PARAMETER-EFFICIENT FINE-TUNING (PEFT){tuple_delimiter}BERT{tuple_delimiter}ACCURACY IMPROVEMENTS{tuple_delimiter}TRUE{tuple_delimiter}2024-01-01T00:00:00{tuple_delimiter}2024-12-31T00:00:00{tuple_delimiter}PEFT was shown to improve the accuracy of BERT models on the GLUE benchmark by 3%, according to a 2024 paper by the AI Research Lab{tuple_delimiter}The new fine-tuning method, Parameter-Efficient Fine-Tuning (PEFT), has shown remarkable improvements in natural language processing tasks.)


-Real Data-
Use the following input for your answer.
Entity specification: {entity_specs}
Claim description: {claim_description}
Text: {input_text}
Output: